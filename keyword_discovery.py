"""
keyword_discovery.py
====================
å‹•æ…‹é«˜è²é‡é—œéµå­—ç™¼ç¾å¼•æ“

åŠŸèƒ½ï¼š
  1. ä¾æ‡‰ç”¨å ´æ™¯ï¼ˆæ—…éŠ / å¥åº· / ç‰™ç§‘ / ä¿å¥å“ï¼‰å¾ç¨®å­é—œéµå­—ä¸­é¸å‡ºè²é‡æœ€é«˜è€…
  2. å°é«˜è²é‡é—œéµå­—æ“´å±•å‡ºé«˜åº¦ç›¸é—œçš„æœå°‹è©
  3. çµæœå¿«å–æ–¼ Supabase keyword_snapshots è¡¨ï¼ŒTTL = 7 å¤©

ä½¿ç”¨ç¯„ä¾‹ï¼š
    from keyword_discovery import run_discovery
    result = run_discovery("æ—…éŠ", geo="TW", top_n=5)
    # result = {
    #   "scenario": "æ—…éŠ",
    #   "geo": "TW",
    #   "top_keywords": [{"keyword": "...", "avg_score": 82.3}, ...],
    #   "related_kws":  [{"keyword": "...", "source": "...", "type": "top|rising"}, ...],
    #   "cached_at": "2026-02-23T04:00:00+00:00",
    #   "from_cache": True/False,
    # }

Supabase å»ºè¡¨ SQLï¼ˆç¬¬ä¸€æ¬¡ä½¿ç”¨å‰è«‹åœ¨ Supabase SQL Editor åŸ·è¡Œï¼‰ï¼š
    create table if not exists keyword_snapshots (
      id            bigint generated always as identity primary key,
      scenario      text        not null,
      geo           text        not null default 'TW',
      top_keywords  jsonb       not null,
      related_kws   jsonb       not null,
      created_at    timestamptz not null default now()
    );
    create index if not exists idx_kw_snap_lookup
      on keyword_snapshots (scenario, geo, created_at desc);
"""

from __future__ import annotations

import logging
import time
from datetime import datetime, timezone, timedelta
from typing import Optional

import pandas as pd
from pytrends.request import TrendReq
from pytrends.exceptions import TooManyRequestsError
from supabase import create_client, Client

logger = logging.getLogger(__name__)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å ´æ™¯ç¨®å­é—œéµå­—ï¼ˆå¯è‡ªè¡Œç·¨è¼¯æˆ–æ“´å……å ´æ™¯ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SCENARIO_SEEDS: dict[str, list[str]] = {
    "æ—…éŠ": [
        "æ—…éŠ",
        "æ©Ÿç¥¨",
        "è¨‚æˆ¿",
        "èƒŒåŒ…å®¢",
        "å‡ºåœ‹",
        "åœ‹å…§æ—…éŠ",
        "æ°‘å®¿",
        "æ—…è¡Œç¤¾",
        "è‡ªç”±è¡Œ",
        "æ—…éŠæ™¯é»",
    ],
    "å¥åº·": [
        "å¥åº·",
        "é¤Šç”Ÿ",
        "é‹å‹•",
        "ç¡çœ ",
        "å¿ƒç†å¥åº·",
        "é£²é£Ÿ",
        "æ¸›é‡",
        "é«”é‡ç®¡ç†",
        "å…ç–«åŠ›",
        "å¥æª¢",
    ],
    "ç‰™ç§‘": [
        "ç‰™ç§‘",
        "ç‰™é†«",
        "çŸ¯æ­£",
        "æ¤ç‰™",
        "æ´—ç‰™",
        "ç‰™å‘¨ç—…",
        "è›€ç‰™",
        "å‡ç‰™",
        "ç‰™é½’ç¾ç™½",
        "éš±é©ç¾",
    ],
    "ä¿å¥å“": [
        "ä¿å¥å“",
        "ç¶­ä»–å‘½",
        "ç›Šç”ŸèŒ",
        "è† åŸè›‹ç™½",
        "é­šæ²¹",
        "è‘‰é»ƒç´ ",
        "éˆ£ç‰‡",
        "ä¿å¥é£Ÿå“",
        "ç‡Ÿé¤Šè£œå……",
        "æŠ—æ°§åŒ–",
    ],
}

# å¿«å–æœ‰æ•ˆæœŸï¼ˆå¤©ï¼‰
CACHE_TTL_DAYS = 7

# pytrends é™é€Ÿç­‰å¾…ï¼ˆç§’ï¼‰
RATE_LIMIT_SLEEP = 60
MAX_RETRIES = 3

# Google Trends åˆ†ææ™‚é–“çª—ï¼ˆç™¼ç¾é«˜è²é‡é—œéµå­—ç”¨ï¼‰
DISCOVERY_TIMEFRAME = "today 1-m"   # è¿‘ 4 é€±

# æ¯æ¬¡ build_payload æœ€å¤šå¯æ”¾å¹¾å€‹é—œéµå­—ï¼ˆGoogle Trends ä¸Šé™ 5ï¼‰
CHUNK_SIZE = 5

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# é—œéµå­— â†’ æœå‹™ / å•†å“æ˜ å°„ï¼ˆç¢ºå®šæ€§é«˜ï¼Œäººå·¥ç­–å±•ï¼‰
# list ä¸­ç¬¬ä¸€å€‹ç‚ºä¸»è¦æœå‹™ï¼Œå…¶å¾Œç‚ºå»¶ä¼¸æ¨è–¦
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
KEYWORD_SERVICE_MAP: dict[str, list[str]] = {
    # â”€â”€ æ—…éŠ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "æ—…éŠ":     ["æ—…éŠå¥—è£è¡Œç¨‹", "æ—…éŠä¿éšª", "æ—…éŠä¿¡ç”¨å¡"],
    "æ©Ÿç¥¨":     ["æ©Ÿç¥¨æ¯”åƒ¹å¹³å°", "å»‰åƒ¹èˆªç©ºç¥¨åˆ¸", "å•†å‹™è‰™å‡ç­‰"],
    "è¨‚æˆ¿":     ["è¨‚æˆ¿å¹³å°ï¼ˆBooking/Agodaï¼‰", "é£¯åº—æ¯”åƒ¹", "æ—©é³¥å„ªæƒ "],
    "èƒŒåŒ…å®¢":   ["é’å¹´æ—…èˆï¼ˆHostelï¼‰", "å»‰åƒ¹æ©Ÿç¥¨", "æ—…éŠè¡Œæç®±"],
    "å‡ºåœ‹":     ["å‡ºåœ‹æ—…éŠä¿éšª", "åœ‹éš›æ¼«éŠæ–¹æ¡ˆ", "æ›åŒ¯æœå‹™"],
    "åœ‹å…§æ—…éŠ": ["åœ‹å…§æ°‘å®¿é è¨‚", "é«˜éµ/å°éµç¥¨åˆ¸", "æ™¯é»é–€ç¥¨"],
    "æ°‘å®¿":     ["æ°‘å®¿é è¨‚å¹³å°", "ç‰¹è‰²æ°‘å®¿é«”é©—", "æ°‘å®¿ç¦®åˆ¸"],
    "æ—…è¡Œç¤¾":   ["å¥—è£æ—…éŠè¡Œç¨‹", "å®¢è£½åŒ–æ—…éŠè¦åŠƒ", "è·Ÿåœ˜æ—…éŠ"],
    "è‡ªç”±è¡Œ":   ["è‡ªç”±è¡Œè¡Œç¨‹è¦åŠƒ", "æ™¯é»ç¥¨åˆ¸", "ç§Ÿè»Šæœå‹™"],
    "æ—…éŠæ™¯é»": ["æ™¯é»é–€ç¥¨é è¨‚", "å°è¦½è§£èªªæœå‹™", "å‘¨é‚Šä½å®¿"],
    "ä¾¿å®œæ©Ÿç¥¨": ["æ©Ÿç¥¨æ¯”åƒ¹å¹³å°", "å»‰åƒ¹èˆªç©ºè¨‚ç¥¨", "Last-minute ç‰¹æƒ "],
    "æ—…éŠæ¨è–¦": ["æ—…éŠéƒ¨è½æ ¼å»£å‘Š", "æ—…éŠ App", "KOL çˆ†æ–™åˆä½œ"],
    "æ—¥æœ¬æ—…éŠ": ["æ—¥æœ¬æ—…éŠå¥—é¤", "JR Pass éµè·¯åˆ¸", "æ—¥æœ¬ SIM å¡"],
    "éŸ“åœ‹æ—…éŠ": ["éŸ“åœ‹æ—…éŠå¥—é¤", "K-ETA é›»å­æ—…è¡Œè¨±å¯", "éŸ“åœ‹ SIM å¡"],
    "æ­æ´²æ—…éŠ": ["æ­æ´²æ—…éŠå¥—é¤", "ç”³æ ¹ä¿éšª", "æ­æ´²ç«è»Šé€šç¥¨"],
    "æ—…éŠä¿éšª": ["æ—…éŠå¹³å®‰éšª", "æµ·å¤–é†«ç™‚éšª", "è¡Œæéºå¤±ç†è³ "],
    "ç§Ÿè»Š":     ["ç§Ÿè»Šå¹³å°", "åœ‹éš›é§•ç…§ç”³è«‹", "GPS ç§Ÿè³ƒ"],

    # â”€â”€ å¥åº· â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "å¥åº·":     ["å¥åº·æª¢æŸ¥å¥—çµ„", "å¥åº·ç®¡ç† App", "å¥åº·è«®è©¢æœå‹™"],
    "é¤Šç”Ÿ":     ["é¤Šç”Ÿé£Ÿå“", "ä¸­é†«èª¿ç†", "é¤Šç”Ÿèª²ç¨‹"],
    "é‹å‹•":     ["å¥èº«æˆ¿æœƒå“¡", "é‹å‹•å™¨æ", "ç·šä¸Šé‹å‹•èª²ç¨‹"],
    "ç¡çœ ":     ["åŠ©çœ æ•é ­/åºŠå¢Š", "ç¡çœ è¿½è¹¤è£ç½®", "åŠ©çœ ç‡Ÿé¤Šå“"],
    "å¿ƒç†å¥åº·": ["å¿ƒç†è«®å•†é ç´„", "å†¥æƒ³ App", "å£“åŠ›ç®¡ç†èª²ç¨‹"],
    "é£²é£Ÿ":     ["å¥åº·é¤ç›’è¨‚é–±", "ç‡Ÿé¤Šè«®è©¢", "é£²é£Ÿè¨˜éŒ„ App"],
    "æ¸›é‡":     ["æ¸›é‡è¨ˆç•«èª²ç¨‹", "ä»£é¤/ç˜¦èº«ç”¢å“", "å¥èº«æ•™ç·´"],
    "é«”é‡ç®¡ç†": ["é«”é‡ç®¡ç†è¨ˆç•«", "ä»£è¬æª¢æ¸¬", "ä½å¡é¤ç›’"],
    "å…ç–«åŠ›":   ["å…ç–«åŠ›ä¿å¥å“", "ç¶­ä»–å‘½ C/D", "ä¸­é†«èª¿è£œ"],
    "å¥æª¢":     ["å¥åº·æª¢æŸ¥å¥—çµ„", "å¥åº·æª¢æŸ¥ä¸­å¿ƒ", "é ç«¯å¥åº·ç›£æ¸¬"],
    "ç‘œçˆ":     ["ç‘œçˆèª²ç¨‹", "ç‘œçˆå¢Š/æœè£", "ç·šä¸Šç‘œçˆè¨‚é–±"],
    "å¥èº«":     ["å¥èº«æˆ¿æœƒç±", "å€‹äººæ•™ç·´", "è›‹ç™½è³ªè£œå……å“"],
    "æ…¢è·‘":     ["è·‘æ­¥é‹", "é‹å‹•è¿½è¹¤è£ç½®", "é¦¬æ‹‰æ¾å ±å"],
    "æ’æ¯’":     ["æ’æ¯’é£²å“", "è…¸é“ä¿å¥å“", "SPA æ’æ¯’ç™‚ç¨‹"],

    # â”€â”€ ç‰™ç§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "ç‰™ç§‘":     ["ç‰™ç§‘è¨ºæ‰€é ç´„", "å£è…”å¥åº·ä¿éšª", "é›»å‹•ç‰™åˆ·"],
    "ç‰™é†«":     ["ç‰™ç§‘è¨ºæ‰€æ¨è–¦", "ç‰™é†«çœ‹è¨ºé ç´„", "ç‰™ç§‘å¥ä¿æ–¹æ¡ˆ"],
    "çŸ¯æ­£":     ["ç‰™é½’çŸ¯æ­£è«®è©¢", "éš±å½¢çŸ¯æ­£ï¼ˆéš±é©ç¾ï¼‰", "çŸ¯æ­£è²»ç”¨ä¼°ç®—"],
    "æ¤ç‰™":     ["æ¤ç‰™æ‰‹è¡“è«®è©¢", "All-on-4 å…¨å£é‡å»º", "æ¤ç‰™åˆ†æœŸä»˜æ¬¾"],
    "æ´—ç‰™":     ["æ´—ç‰™é ç´„", "è¶…éŸ³æ³¢æ½”ç‰™", "å±…å®¶æ½”ç‰™çµ„"],
    "ç‰™å‘¨ç—…":   ["ç‰™å‘¨ç—…æ²»ç™‚", "ç‰™å‘¨é›·å°„ç™‚ç¨‹", "ç‰™å‘¨ä¿é¤Šå“"],
    "è›€ç‰™":     ["è›€ç‰™å¡«è£œ/æ ¹ç®¡æ²»ç™‚", "å…’ç«¥ç‰™ç§‘", "é˜²è›€ç‰™è†"],
    "å‡ç‰™":     ["é™¶ç“·å‡ç‰™", "æ´»å‹•å‡ç‰™", "å…¨ç“·å† ä¿®å¾©"],
    "ç‰™é½’ç¾ç™½": ["å†·å…‰ç¾ç™½ç™‚ç¨‹", "å±…å®¶ç¾ç™½è²¼ç‰‡", "ç¾ç™½ç‰™è†"],
    "éš±é©ç¾":   ["éš±é©ç¾çŸ¯æ­£è«®è©¢", "Invisalign å¥—çµ„", "é€æ˜çŸ¯æ­£å™¨"],
    "ç‰™å¥—":     ["é‡‘å±¬çŸ¯æ­£ç‰™å¥—", "é™¶ç“·ç‰™å¥—", "å¤œé–“ç£¨ç‰™é˜²è­·å¥—"],
    "ç‰™çµçŸ³":   ["ç‰™çµçŸ³æ¸…é™¤", "è¶…éŸ³æ³¢æ´—ç‰™", "æŠ‘èŒæ¼±å£æ°´"],

    # â”€â”€ ä¿å¥å“ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "ä¿å¥å“":   ["ç¶œåˆä¿å¥å“æ–¹æ¡ˆ", "ä¿å¥å“è¨‚é–±ç›’", "ä¿å¥å“æ¯”åƒ¹å¹³å°"],
    "ç¶­ä»–å‘½":   ["ç¶œåˆç¶­ä»–å‘½", "ç¶­ä»–å‘½ D3/K2", "å…’ç«¥ç¶­ä»–å‘½è»Ÿç³–"],
    "ç›Šç”ŸèŒ":   ["ç›Šç”ŸèŒè† å›Š", "ç›Šç”ŸèŒé£²å“", "è…¸é“èŒç›¸æª¢æ¸¬"],
    "è† åŸè›‹ç™½": ["è† åŸè›‹ç™½ç²‰/é£²", "å£æœç¾å®¹ä¿å¥å“", "æŠ—è€åŒ–çµ„åˆ"],
    "é­šæ²¹":     ["Omega-3 é­šæ²¹", "æ·±æµ·é­šæ²¹è† å›Š", "å…’ç«¥é­šæ²¹"],
    "è‘‰é»ƒç´ ":   ["è‘‰é»ƒç´ è­·çœ¼è† å›Š", "è‘‰é»ƒç´ é£²å“", "3C è­·çœ¼çµ„åˆ"],
    "éˆ£ç‰‡":     ["éˆ£+D3 è£œå……å“", "å…’ç«¥æˆé•·éˆ£", "è€å¹´éª¨éª¼ä¿å¥"],
    "ä¿å¥é£Ÿå“": ["åŠŸèƒ½æ€§ä¿å¥é£Ÿå“", "æœ‰æ©Ÿä¿å¥å“", "å°ç£è£½ä¿å¥å“"],
    "ç‡Ÿé¤Šè£œå……": ["é‹å‹•ç‡Ÿé¤Šå“", "è¡“å¾Œç‡Ÿé¤Šè£œå……", "å…¨æ–¹ä½è¤‡åˆç¶­ç”Ÿç´ "],
    "æŠ—æ°§åŒ–":   ["æŠ—æ°§åŒ–ä¿å¥å“ï¼ˆQ10ï¼‰", "ç™½è—œè˜†é†‡", "ç¶­ä»–å‘½ C é«˜åŠ‘é‡"],
    "è›‹ç™½è³ª":   ["ä¹³æ¸…è›‹ç™½", "æ¤ç‰©æ€§è›‹ç™½ç²‰", "é«˜è›‹ç™½é£²é£Ÿè¨ˆç•«"],
    "è–‘é»ƒ":     ["è–‘é»ƒè† å›Š", "è–‘é»ƒæ‹¿éµ", "æ¶ˆç‚æŠ—æ°§åŒ–çµ„åˆ"],
}

# å ´æ™¯å±¤ç´šå‚™ç”¨ï¼ˆé—œéµå­—ä¸åœ¨ KEYWORD_SERVICE_MAP æ™‚ä½¿ç”¨ï¼‰
SCENARIO_SERVICE_FALLBACK: dict[str, list[str]] = {
    "æ—…éŠ":  ["æ—…éŠè¦åŠƒæœå‹™", "ä½å®¿é è¨‚", "æ—…éŠä¿éšª"],
    "å¥åº·":  ["å¥åº·æª¢æŸ¥", "ä¿å¥å“", "å¥èº«èª²ç¨‹"],
    "ç‰™ç§‘":  ["ç‰™ç§‘è¨ºæ‰€è«®è©¢", "å£è…”ä¿å¥å“", "çŸ¯æ­£è©•ä¼°"],
    "ä¿å¥å“": ["ä¿å¥å“è¨‚é–±", "ç‡Ÿé¤Šè«®è©¢", "å¥åº·ç®¡ç†"],
}


def get_keyword_services(keyword: str, scenario: str = "") -> list[str]:
    """
    å›å‚³è©²é—œéµå­—å°æ‡‰çš„æœå‹™/å•†å“æ¨æ¸¬ã€‚
    å„ªå…ˆç²¾ç¢ºåŒ¹é…ï¼Œå…¶æ¬¡æ¨¡ç³ŠåŒ¹é…ï¼ˆåŒ…å«é—œä¿‚ï¼‰ï¼Œæœ€å¾Œç”¨å ´æ™¯å‚™ç”¨ã€‚
    """
    # 1. ç²¾ç¢ºåŒ¹é…
    if keyword in KEYWORD_SERVICE_MAP:
        return KEYWORD_SERVICE_MAP[keyword]

    # 2. æ¨¡ç³ŠåŒ¹é…ï¼ˆé—œéµå­—åŒ…å«å·²çŸ¥è© or å·²çŸ¥è©åŒ…å«é—œéµå­—ï¼‰
    for key, svcs in KEYWORD_SERVICE_MAP.items():
        if key in keyword or keyword in key:
            return svcs

    # 3. å ´æ™¯å‚™ç”¨
    if scenario and scenario in SCENARIO_SERVICE_FALLBACK:
        return SCENARIO_SERVICE_FALLBACK[scenario]

    return []



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Supabase é€£ç·šï¼ˆlazy singletonï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SUPABASE_URL = "https://shiqrelmuvzwcxqndnyq.supabase.co"
SUPABASE_KEY = (
    "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9"
    ".eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InNoaXFyZWxt"
    "dXZ6d2N4cW5kbnlxIiwicm9sZSI6InNlcnZpY2Vfcm9sZ"
    "SIsImlhdCI6MTczODMwNjk0MiwiZXhwIjoyMDUzODgyOTQyfQ"
    ".xiA87hhy0tOTytDmSmy_pRqeqVSLtEBqsrTxrvLy0ec"
)

_sb_client: Optional[Client] = None


def _get_supabase() -> Client:
    global _sb_client
    if _sb_client is None:
        _sb_client = create_client(SUPABASE_URL, SUPABASE_KEY)
        logger.info("Supabase é€£ç·šå»ºç«‹ï¼ˆkeyword_discoveryï¼‰")
    return _sb_client


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# pytrends é€£ç·šï¼ˆlazy singletonï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_pt_client: Optional[TrendReq] = None


def _get_pytrends() -> TrendReq:
    global _pt_client
    if _pt_client is None:
        _pt_client = TrendReq(
            hl="zh-TW",
            tz=-480,
            timeout=(10, 30),
            retries=2,
            backoff_factor=1.5,
        )
        logger.info("TrendReq åˆå§‹åŒ–å®Œæˆï¼ˆkeyword_discoveryï¼‰")
    return _pt_client


def _safe_call(fn, *args, **kwargs):
    """å‘¼å« pytrends APIï¼Œé‡åˆ° 429 é™é€Ÿå‰‡ç­‰å¾…å¾Œé‡è©¦ã€‚"""
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            return fn(*args, **kwargs)
        except TooManyRequestsError:
            if attempt == MAX_RETRIES:
                raise
            logger.warning(
                "Google é™é€Ÿ (429)ï¼Œç¬¬ %d/%d æ¬¡é‡è©¦ï¼Œç­‰å¾… %ds â€¦",
                attempt, MAX_RETRIES, RATE_LIMIT_SLEEP,
            )
            time.sleep(RATE_LIMIT_SLEEP)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 1ï¼šå¾ç¨®å­é—œéµå­—ä¸­é¸å‡ºé«˜è²é‡é—œéµå­—
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def discover_top_keywords(
    scenario: str,
    geo: str = "TW",
    top_n: int = 5,
) -> list[dict]:
    """
    å°‡å ´æ™¯ç¨®å­é—œéµå­—åˆ†æ‰¹é€å…¥ Google Trends interest_over_timeï¼Œ
    è¨ˆç®—è¿‘ 4 é€±å¹³å‡æœå°‹è²é‡ï¼Œå›å‚³è²é‡æœ€é«˜çš„ top_n ç­†ã€‚

    Returns:
        [{"keyword": "æ—…éŠ", "avg_score": 82.3}, ...]  å·²æŒ‰ avg_score é™åºæ’åˆ—
    """
    seeds = SCENARIO_SEEDS.get(scenario)
    if not seeds:
        raise ValueError(f"æœªçŸ¥å ´æ™¯ï¼š'{scenario}'ï¼Œå¯ç”¨å ´æ™¯ï¼š{list(SCENARIO_SEEDS.keys())}")

    pt = _get_pytrends()
    scores: dict[str, float] = {}

    # æ¯æ‰¹æœ€å¤š CHUNK_SIZE å€‹ï¼ˆGoogle Trends é™åˆ¶ï¼‰
    chunks = [seeds[i: i + CHUNK_SIZE] for i in range(0, len(seeds), CHUNK_SIZE)]

    for idx, chunk in enumerate(chunks):
        logger.info("  [%s] æ‰¹æ¬¡ %d/%dï¼Œé—œéµå­—ï¼š%s", scenario, idx + 1, len(chunks), chunk)
        try:
            pt.build_payload(kw_list=chunk, timeframe=DISCOVERY_TIMEFRAME, geo=geo)
            df: pd.DataFrame = _safe_call(pt.interest_over_time)

            if df is not None and not df.empty:
                if "isPartial" in df.columns:
                    df = df.drop(columns=["isPartial"])
                for kw in chunk:
                    if kw in df.columns:
                        scores[kw] = round(float(df[kw].mean()), 2)
                    else:
                        scores[kw] = 0.0
            else:
                for kw in chunk:
                    scores[kw] = 0.0

        except Exception as exc:
            logger.warning("  æ‰¹æ¬¡ %s å¤±æ•—ï¼Œç•¥éï¼š%s", chunk, exc)
            for kw in chunk:
                scores[kw] = 0.0

        # æ‰¹æ¬¡ä¹‹é–“ç¨ä½œåœé “
        if idx < len(chunks) - 1:
            time.sleep(3)

    # æ’åºä¸¦å– top_n
    sorted_kws = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_n]
    result = [{"keyword": kw, "avg_score": score} for kw, score in sorted_kws]
    logger.info("  [%s] Top %d é«˜è²é‡é—œéµå­—ï¼š%s", scenario, top_n, result)
    return result


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 2ï¼šæ“´å±•ç›¸é—œé—œéµå­—
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def expand_related_keywords(
    top_keywords: list[dict],
    geo: str = "TW",
    max_per_kw: int = 10,
) -> list[dict]:
    """
    å°æ¯å€‹é«˜è²é‡é—œéµå­—å‘¼å« related_queriesï¼Œå– top + rising å„æœ€å¤š max_per_kw ç­†ï¼Œ
    å»é‡å¾Œå›å‚³ã€‚

    Returns:
        [{"keyword": "ä¾¿å®œæ©Ÿç¥¨", "source": "æ©Ÿç¥¨", "type": "top", "value": 100}, ...]
    """
    pt = _get_pytrends()
    seen: set[str] = set()
    related: list[dict] = []

    kw_list = [item["keyword"] for item in top_keywords]

    # related_queries æ¯æ¬¡æœ€å¤š 5 å€‹é—œéµå­—ï¼Œé€™è£¡å·²å‡è¨­ top_n â‰¤ 5
    # è‹¥ top_n > 5ï¼Œåˆ†æ‰¹è™•ç†
    chunks = [kw_list[i: i + CHUNK_SIZE] for i in range(0, len(kw_list), CHUNK_SIZE)]

    for idx, chunk in enumerate(chunks):
        logger.info("  [related] æ‰¹æ¬¡ %d/%dï¼Œé—œéµå­—ï¼š%s", idx + 1, len(chunks), chunk)
        try:
            pt.build_payload(kw_list=chunk, timeframe=DISCOVERY_TIMEFRAME, geo=geo)
            result: dict = _safe_call(pt.related_queries)

            for kw in chunk:
                kw_data = result.get(kw, {}) if result else {}

                for qtype in ("top", "rising"):
                    df = kw_data.get(qtype)
                    if df is None or df.empty:
                        continue
                    df = df.head(max_per_kw)
                    for _, row in df.iterrows():
                        q = str(row.get("query", "")).strip()
                        if q and q not in seen:
                            seen.add(q)
                            related.append({
                                "keyword": q,
                                "source": kw,
                                "type": qtype,
                                "value": int(row.get("value", 0)),
                            })

        except Exception as exc:
            logger.warning("  related_queries æ‰¹æ¬¡ %s å¤±æ•—ï¼š%s", chunk, exc)

        if idx < len(chunks) - 1:
            time.sleep(3)

    logger.info("  ç›¸é—œé—œéµå­—å…± %d ç­†ï¼ˆå»é‡å¾Œï¼‰", len(related))
    return related


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å¿«å–å­˜å–ï¼ˆSupabaseï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _load_cache(scenario: str, geo: str) -> Optional[dict]:
    """
    è‹¥ Supabase å…§æœ‰è©²å ´æ™¯çš„æ–°é®®å¿«ç…§ï¼ˆ< CACHE_TTL_DAYS å¤©ï¼‰ï¼Œå›å‚³ä¹‹ï¼›å¦å‰‡ Noneã€‚
    """
    try:
        sb = _get_supabase()
        ttl_cutoff = (
            datetime.now(timezone.utc) - timedelta(days=CACHE_TTL_DAYS)
        ).isoformat()

        resp = (
            sb.table("keyword_snapshots")
            .select("id, top_keywords, related_kws, created_at")
            .eq("scenario", scenario)
            .eq("geo", geo)
            .gte("created_at", ttl_cutoff)
            .order("created_at", desc=True)
            .limit(1)
            .execute()
        )

        if resp.data:
            row = resp.data[0]
            logger.info(
                "å¿«å–å‘½ä¸­ï¼šå ´æ™¯=%s, geo=%s, å»ºç«‹æ–¼ %s",
                scenario, geo, row["created_at"],
            )
            return row
        return None

    except Exception as exc:
        logger.warning("è®€å– Supabase å¿«å–å¤±æ•—ï¼ˆå°‡é‡æ–°æŠ“å–ï¼‰ï¼š%s", exc)
        return None


def _save_cache(
    scenario: str,
    geo: str,
    top_keywords: list[dict],
    related_kws: list[dict],
) -> str:
    """
    å°‡ç™¼ç¾çµæœå¯«å…¥ Supabase keyword_snapshotsï¼Œå›å‚³ created_at æ™‚é–“å­—ä¸²ã€‚
    """
    now_iso = datetime.now(timezone.utc).isoformat()
    try:
        sb = _get_supabase()
        sb.table("keyword_snapshots").insert({
            "scenario": scenario,
            "geo": geo,
            "top_keywords": top_keywords,
            "related_kws": related_kws,
            "created_at": now_iso,
        }).execute()
        logger.info("å¿«å–å·²å„²å­˜ï¼šå ´æ™¯=%s, geo=%s", scenario, geo)
    except Exception as exc:
        logger.warning("å„²å­˜ Supabase å¿«å–å¤±æ•—ï¼ˆä¸å½±éŸ¿ä¸»åŠŸèƒ½ï¼‰ï¼š%s", exc)
    return now_iso


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¸»è¦å°å¤– API
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_discovery(
    scenario: str,
    geo: str = "TW",
    top_n: int = 5,
    force_refresh: bool = False,
) -> dict:
    """
    ä¸»è¦å…¥å£ï¼šç™¼ç¾ç‰¹å®šå ´æ™¯çš„é«˜è²é‡é—œéµå­—åŠå…¶ç›¸é—œé—œéµå­—ã€‚

    Args:
        scenario:      å ´æ™¯åç¨±ï¼Œé ˆå­˜åœ¨æ–¼ SCENARIO_SEEDSï¼ˆæ—…éŠ / å¥åº· / ç‰™ç§‘ / ä¿å¥å“ï¼‰
        geo:           Google Trends åœ°å€ä»£ç¢¼ï¼ˆé è¨­ TWï¼‰
        top_n:         å–å¹¾å€‹é«˜è²é‡é—œéµå­—ï¼ˆé è¨­ 5ï¼Œæœ€å¤§å»ºè­° 5 ä»¥ç¬¦åˆ API é™åˆ¶ï¼‰
        force_refresh: True æ™‚å¿½ç•¥å¿«å–ï¼Œå¼·åˆ¶é‡æ–°å¾ Google Trends æŠ“å–

    Returns:
        {
          "scenario": str,
          "geo": str,
          "top_keywords":  [{"keyword": str, "avg_score": float}, ...],
          "related_kws":   [{"keyword": str, "source": str, "type": str, "value": int}, ...],
          "cached_at": str (ISO 8601),
          "from_cache": bool,
        }
    """
    if scenario not in SCENARIO_SEEDS:
        raise ValueError(
            f"æœªçŸ¥å ´æ™¯ï¼š'{scenario}'ï¼Œå¯ç”¨å ´æ™¯ï¼š{list(SCENARIO_SEEDS.keys())}"
        )

    # 1. å˜—è©¦å¿«å–
    if not force_refresh:
        cached = _load_cache(scenario, geo)
        if cached:
            # å¿«å–å‘½ä¸­æ™‚å³æ™‚è£œå…… servicesï¼ˆä¸å­˜åœ¨å¿«å–ä¸­ï¼Œç¢ºä¿ map æ›´æ–°ç«‹å³ç”Ÿæ•ˆï¼‰
            top_kws = [
                {**kw, "services": get_keyword_services(kw["keyword"], scenario)}
                for kw in cached["top_keywords"]
            ]
            rel_kws = [
                {**kw, "services": get_keyword_services(kw["keyword"], scenario)}
                for kw in cached["related_kws"]
            ]
            return {
                "scenario": scenario,
                "geo": geo,
                "top_keywords": top_kws,
                "related_kws": rel_kws,
                "cached_at": cached["created_at"],
                "from_cache": True,
            }

    logger.info("é–‹å§‹ç™¼ç¾å ´æ™¯ [%s]ï¼ˆgeo=%s, top_n=%dï¼‰â€¦", scenario, geo, top_n)

    # 2. é«˜è²é‡é—œéµå­—ç™¼ç¾
    top_keywords_raw = discover_top_keywords(scenario, geo=geo, top_n=top_n)

    time.sleep(3)   # é¿å…é€£çºŒå‘¼å«è§¸ç™¼é™é€Ÿ

    # 3. ç›¸é—œé—œéµå­—æ“´å±•
    related_kws_raw = expand_related_keywords(top_keywords_raw, geo=geo)

    # 4. å¯«å…¥ Supabase å¿«å–ï¼ˆä¸å« servicesï¼Œè®“ map æ›´æ–°å¯ç«‹å³åæ˜ ï¼‰
    cached_at = _save_cache(scenario, geo, top_keywords_raw, related_kws_raw)

    # 5. å³æ™‚æ³¨å…¥ services æ¬„ä½
    top_keywords = [
        {**kw, "services": get_keyword_services(kw["keyword"], scenario)}
        for kw in top_keywords_raw
    ]
    related_kws = [
        {**kw, "services": get_keyword_services(kw["keyword"], scenario)}
        for kw in related_kws_raw
    ]

    return {
        "scenario": scenario,
        "geo": geo,
        "top_keywords": top_keywords,
        "related_kws": related_kws,
        "cached_at": cached_at,
        "from_cache": False,
    }


def list_scenarios() -> list[str]:
    """å›å‚³æ‰€æœ‰å¯ç”¨å ´æ™¯åç¨±ã€‚"""
    return list(SCENARIO_SEEDS.keys())


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI å¿«é€Ÿæ¸¬è©¦
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import json
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    import sys
    scenario_arg = sys.argv[1] if len(sys.argv) > 1 else "æ—…éŠ"
    force_arg = "--force" in sys.argv

    print(f"\n{'='*55}")
    print(f"  Keyword Discovery â€” å ´æ™¯ï¼š{scenario_arg}")
    print(f"{'='*55}\n")

    r = run_discovery(scenario_arg, geo="TW", top_n=5, force_refresh=force_arg)

    print(f"\nâœ… ä¾†æºï¼š{'å¿«å–' if r['from_cache'] else 'å³æ™‚æŠ“å–'}ï¼ˆ{r['cached_at']}ï¼‰")
    print("\nğŸ“Š é«˜è²é‡é—œéµå­—ï¼š")
    for i, kw in enumerate(r["top_keywords"], 1):
        print(f"  {i}. {kw['keyword']:15s}  avg_score={kw['avg_score']}")

    print(f"\nğŸ”— ç›¸é—œé—œéµå­—ï¼ˆå…± {len(r['related_kws'])} ç­†ï¼‰ï¼š")
    for kw in r["related_kws"][:15]:
        tag = "ğŸ”¼" if kw["type"] == "rising" else "ğŸ”¸"
        print(f"  {tag} {kw['keyword']:20s}  ä¾†æº={kw['source']}  å€¼={kw['value']}")
    if len(r["related_kws"]) > 15:
        print(f"  â€¦ é‚„æœ‰ {len(r['related_kws'])-15} ç­†")
